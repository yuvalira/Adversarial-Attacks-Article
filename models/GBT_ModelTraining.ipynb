{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LIZ03DpO5Wf4",
        "QRXPEpwndR6M",
        "MLdPvOpRuLy1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuvalira/Final-Project-Adversarial-Attack-on-Tabular-Classification/blob/main/GBT/GBT_ModelTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BAmAJ1SB4Prh"
      },
      "outputs": [],
      "source": [
        "# Standard library\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Data loading\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Data handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "# Scikit-learn: Preprocessing, Modeling, Evaluation\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    roc_curve\n",
        ")\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U6mUNht8_vr"
      },
      "source": [
        "#**Classical Model**\n",
        "In this section, we implement a classical tree-based machine learning model to serve as a performance baseline for comparison against Large Language Models (LLMs) on a tabular classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-learn Gradient Boosted Trees (GBT)\n",
        "\n",
        "This project uses the Gradient Boosted Trees (GBT) implementation from the popular scikit-learn machine learning library.  \n",
        "GBT is a powerful ensemble learning method that builds models by combining many weak learners (decision trees) into a strong predictive model.  \n",
        "We selected this approach because it integrates well with tabular data, provides strong performance, and avoids the current compatibility limitations of TensorFlow Decision Forests with Keras 3.\n",
        "\n",
        "There are other existing implementations of GBT, including TensorFlow Decision Forests [1], XGBoost [2], and LightGBM [3].  \n",
        "We chose to implement the model using scikit-learn [0] due to its simplicity, stability, and excellent compatibility with research workflows in Python.\n",
        "\n",
        "References:\n",
        "\n",
        "[0] Scikit-learn: https://scikit-learn.org/stable/  \n",
        "[1] TensorFlow Decision Forests: https://www.tensorflow.org/decision_forests  \n",
        "[2] XGBoost: https://xgboost.readthedocs.io/  \n",
        "[3] LightGBM: https://lightgbm.readthedocs.io/\n"
      ],
      "metadata": {
        "id": "C06aJBdr5LwP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIZ03DpO5Wf4"
      },
      "source": [
        "### 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-learn"
      ],
      "metadata": {
        "id": "p_AEghQd5xtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRXPEpwndR6M"
      },
      "source": [
        "### 2. Data Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading data-frames from Huggingface"
      ],
      "metadata": {
        "id": "-J6kiBJL6v4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_id = \"\" # Enter dataset repo\n",
        "\n",
        "# Read train dataset\n",
        "csv_filename = \"train.csv\"\n",
        "csv_path = hf_hub_download(repo_id=repo_id, filename=csv_filename)\n",
        "train_data = pd.read_csv(csv_path)\n",
        "\n",
        "# Read validation dataset\n",
        "csv_filename = \"val.csv\"\n",
        "csv_path = hf_hub_download(repo_id=repo_id, filename=csv_filename)\n",
        "val_data = pd.read_csv(csv_path)\n",
        "\n",
        "# Read test dataset\n",
        "csv_filename = \"test.csv\"\n",
        "csv_path = hf_hub_download(repo_id=repo_id, filename=csv_filename)\n",
        "test_data = pd.read_csv(csv_path)"
      ],
      "metadata": {
        "id": "LxcN8VzEdYIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9X8aaF8vf2i"
      },
      "source": [
        "First, we'll track which columns have numerical values and which are categorical. We'll also pay special attention to the target column and weight column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztICFE28Lrce"
      },
      "outputs": [],
      "source": [
        "# Identify numerical columns (int, float)\n",
        "numerical_columns = train_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Identify categorical columns (object, category)\n",
        "categorical_columns = train_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Print the feature names (column names)\n",
        "print(\"Numerical Features:\", numerical_columns)\n",
        "print(\"Categorical Features:\", categorical_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjJVrRkKLqMW"
      },
      "outputs": [],
      "source": [
        "# Define the columns based on the categories you mentioned\n",
        "TARGET_COLUMN_NAME = \"\"  # The column we're predicting\n",
        "TARGET_LABELS = []  # Enter target labels\n",
        "\n",
        "# Numeric features based on the output\n",
        "NUMERIC_FEATURE_NAMES = [] # Enter numeric feature\n",
        "\n",
        "# Categorical features based on the output\n",
        "CATEGORICAL_FEATURE_NAMES = [] # Enter categorical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O7dD_kGhxfn"
      },
      "source": [
        "Now, we Create copies of dataframes to work on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1305CejrxijP"
      },
      "outputs": [],
      "source": [
        "# Create copies to work on\n",
        "train_data = train_data.copy()\n",
        "val_data = val_data.copy()\n",
        "test_data = test_data.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LGLcHifwu3Q"
      },
      "source": [
        "Now we will show the shapes of the training and test dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0-FYIJGx1xW"
      },
      "outputs": [],
      "source": [
        "print(f\"Train data shape: {train_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")\n",
        "print(f\"Validation data shape: {val_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T65T-HLwfGH"
      },
      "source": [
        "The target column (`income`) is first converted from string labels (`'<=50K'`, `'>50K'`) to numeric values (0, 1) for compatibility with scikit-learn.\n",
        "\n",
        "In addition, all categorical feature columns are cast to string type to ensure consistency and avoid potential dtype issues during encoding.\n",
        "\n",
        "This prepares the dataset for subsequent Label Encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77yk5jFUws1A"
      },
      "outputs": [],
      "source": [
        "def prepare_dataframe(df):\n",
        "    # Convert the target labels from string to integer.\n",
        "    df[TARGET_COLUMN_NAME] = df[TARGET_COLUMN_NAME].map(\n",
        "        TARGET_LABELS.index\n",
        "    )\n",
        "    # Cast the categorical features to string.\n",
        "    for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
        "        df[feature_name] = df[feature_name].astype(str)\n",
        "\n",
        "\n",
        "prepare_dataframe(train_data)\n",
        "prepare_dataframe(val_data)\n",
        "prepare_dataframe(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training the Gradient Boosted Trees (GBT) model, it is necessary to convert all categorical (non-numeric) features into numerical format, since scikit-learn models cannot process string data directly.\n",
        "\n",
        "In this step, we apply Label Encoding to all categorical features. Label Encoding assigns a unique integer value to each category within a feature.\n",
        "\n",
        "To avoid data leakage, we:\n",
        "\n",
        "1. fit the encoder only on the training set.\n",
        "\n",
        "2. apply (transform) the same encoding to the validation and test sets"
      ],
      "metadata": {
        "id": "kc1EGs8W9Q56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save encoders to reuse\n",
        "encoders = {}\n",
        "\n",
        "for col in CATEGORICAL_FEATURE_NAMES:\n",
        "    # Combine all values for fitting\n",
        "    combined_values = pd.concat([train_data[col], val_data[col], test_data[col]], axis=0)\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    le.fit(combined_values)  # Fit on all available data (to cover all categories)\n",
        "\n",
        "    # Transform individually to preserve dataset structure\n",
        "    train_data[col] = le.transform(train_data[col])\n",
        "    val_data[col] = le.transform(val_data[col])\n",
        "    test_data[col] = le.transform(test_data[col])\n",
        "\n",
        "    encoders[col] = le  # Save for reuse\n",
        "\n",
        "print(\"All categorical features encoded successfully.\")"
      ],
      "metadata": {
        "id": "lx5DmtQj9WJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3_vYU1yI7y93"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLdPvOpRuLy1"
      },
      "source": [
        "### 3. Model Training & Evaluation\n",
        "We'll train a **Gradient Boosted Trees (GBT)** model using the `GradientBoostingClassifier` from the scikit-learn library.\n",
        "\n",
        "This model is chosen because:\n",
        "- It improves predictive performance over standard decision trees.\n",
        "- It reduces overfitting by sequentially training trees, each correcting the errors of the previous one.\n",
        "- It provides flexibility to handle both numerical and encoded categorical features (after preprocessing).\n",
        "- It integrates seamlessly with the scikit-learn ecosystem and works efficiently for tabular data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ8b6R1R0NqT"
      },
      "outputs": [],
      "source": [
        "# Gradient Boosted Trees (GBT) model for classification with configured hyperparameters\n",
        "\n",
        "model_gbt = GradientBoostingClassifier(\n",
        "    n_estimators=250,       # Number of boosting stages (trees)\n",
        "    max_depth=5,            # Maximum depth of each tree\n",
        "    min_samples_leaf=6,     # Minimum number of samples required at a leaf node\n",
        "    subsample=0.65,         # Fraction of samples to be used for fitting each base learner\n",
        "    random_state=42         # For reproducibility\n",
        ")\n",
        "\n",
        "print(\"GBT model initialized successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBj3lk7vSTQb"
      },
      "source": [
        "The run_experiment() method separates features and target labels from the datasets, trains the scikit-learn Gradient Boosting Classifier on the training set, and evaluates its performance on the validation and test sets.\n",
        "\n",
        "The function calculates standard classification metrics, including accuracy, precision, recall, F1-score, and ROC-AUC.\n",
        "\n",
        "Training time is also measured to assess computational efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovm3QNCVTpGA"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model, train_data, val_data, test_data):\n",
        "    # Exclude target\n",
        "    exclude_columns = [TARGET_COLUMN_NAME]\n",
        "\n",
        "    # Separate features and labels\n",
        "    X_train = train_data.drop(columns=exclude_columns)\n",
        "    y_train = train_data[TARGET_COLUMN_NAME]\n",
        "\n",
        "    X_val = val_data.drop(columns=exclude_columns)\n",
        "    y_val = val_data[TARGET_COLUMN_NAME]\n",
        "\n",
        "    X_test = test_data.drop(columns=exclude_columns)\n",
        "    y_test = test_data[TARGET_COLUMN_NAME]\n",
        "\n",
        "    # Train the model\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    # Evaluate on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Correctly Predicted\n",
        "    correct_predictions = np.sum(y_test == y_pred)\n",
        "    total_predictions = len(y_test)\n",
        "    print(f\"Correctly Predicted: {correct_predictions}/{total_predictions}\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "\n",
        "    print(f\"Model evaluation complete:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"ROC AUC: {auc:.4f}\")\n",
        "    print(f\"Training Time: {training_time:.4f} seconds\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_gbt = run_experiment(model_gbt, train_data, val_data, test_data)"
      ],
      "metadata": {
        "id": "MmDKAJbJBJuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We extract and display the relative importance of each feature using the trained model's `feature_importances_` attribute."
      ],
      "metadata": {
        "id": "nrhP0ZkZHlQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance analysis for sklearn Gradient Boosting\n",
        "feature_names = train_data.drop(columns=[TARGET_COLUMN_NAME]).columns\n",
        "importances = model_gbt.feature_importances_\n",
        "\n",
        "feature_importance_dict = dict(zip(feature_names, importances))\n",
        "sorted_importances = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\" Feature Importances:\")\n",
        "for feature, importance in sorted_importances:\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "id": "oIaS_JW3Gldc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Receiver Operating Characteristic (ROC) Curve:\n",
        "\n",
        "ROC curve is a graphical representation of a classifier's performance across different classification thresholds.  \n",
        "It plots the **True Positive Rate (Sensitivity)** against the **False Positive Rate (1 - Specificity)** at various threshold settings.  \n",
        "The ROC curve helps to evaluate the trade-off between sensitivity and specificity.  \n",
        "A model with a curve closer to the top-left corner indicates better performance.  \n",
        "The **Area Under the Curve (AUC)** summarizes the overall ability of the model to distinguish between the classes, where an AUC of 1.0 represents a perfect classifier and 0.5 represents random guessing."
      ],
      "metadata": {
        "id": "x41Tgaa3InYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare test data\n",
        "X_test = test_data.drop(columns=[TARGET_COLUMN_NAME])\n",
        "y_true = test_data[TARGET_COLUMN_NAME].values\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_proba = model_gbt.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC AUC score\n",
        "roc_auc = roc_auc_score(y_true, y_proba)\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "\n",
        "# Compute ROC curve points\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.4f})\", linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Classifier (AUC = 0.5)\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bahOrx0TH7eF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}